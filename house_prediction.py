# -*- coding: utf-8 -*-
"""house prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nYGVd3qu8RpylE45ZDYh6p03PK6t7xwz
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import copy
import math

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Housing.csv')
df.head()

df.tail(10)

df.shape

df.info()

df.isnull().sum()

df.describe().transpose()

df.duplicated().sum()

plt.hist(df['price'],bins=20,edgecolor='red')
plt.title('Distribution of House Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

int_colos=df.select_dtypes(include='number').columns
sns.heatmap(df[int_colos].corr(),annot=True)

data=df.copy

x=df.drop('price',axis=1)
y=df['price']
x

x=pd.get_dummies(x,drop_first=True)
x=x.astype(int)
x=x.to_numpy()
y=y.to_numpy()

print(x[:10])

def zscore_normalize_features(x):
  mu=np.mean(x,axis=0)
  sigma=np.std(x,axis=0)
  x=(x-mu)/sigma
  return x

print(x[:10])
zscore_normalize_features(x)
print(x[:10])

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)

print(f"shape of x_train{x_train.shape},shape of y_train{y_train.shape}")
print(f"shape of x-test{x_test.shape},shape of y_test{y_test.shape}")

def compute_cost(x,y,w,b):
  m=x.shape[0]
  cost=0.0
  for i in range (m):
    f_wb_i=np.dot(x[i],w)+b
    cost=cost+(f_wb_i-y[i])**2
    cost=cost/(2*m)
    return cost

n=x_train.shape[1]
w=np.zeros(n)
b=0
compute_cost(x_train,y_train,w,b)

def compute_gradient(x,y,w,b):
  #Taking Derivatives
  m,n=x.shape

  dj_dw=np.zeros(n)
  dj_db=0
  for i in range(m):
    f_wb_i=np.dot(x[i],w)+b
    dj_db_i=f_wb_i-y[i]
    dj_dw_i=(f_wb_i-y[i])*x[i]
    dj_db+=dj_db_i
    dj_dw+=dj_dw_i

  dj_dw=dj_dw/m
  dj_db=dj_db/m

  return dj_db,dj_dw

gradient=compute_gradient(x_train,y_train,w,b)
print(gradient)

x_train.shape[1]

def gradient_descent(x,y,w_in,b_in,compute_cost,compute_gradient,alpha,iterations):

    J_history=[]
    w=copy.deepcopy(w_in)
    b=b_in
    for i in range(iterations):
        dj_db,dj_dw=compute_gradient(x,y,w,b)
        w=w-(alpha*dj_dw)
        b=b-(alpha*dj_db)

        if i<100000:
            cost=compute_cost(x,y,w,b)
            J_history.append(cost)
        if i%math.ceil(iterations/10)==0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}")
    return w,b,J_history

iterations=10000
alpha=0.01
w_final,b_final,j_history=gradient_descent(x_train,y_train,w,b,compute_cost,compute_gradient,alpha,iterations)

print(w_final,b_final)

fig, (ax1,ax2)=plt.subplots(1,2,constrained_layout=True,figsize=(12,4))
ax1.plot(j_history)
ax2.plot(100 + np.arange(len(j_history[100:])), j_history[100:])

ax1.set_title("cost vs iteration")
ax2.set_title("cost vs iteration (tai)")
ax1.set_xlabel('cost')
ax2.set_xlabel('cost')
ax1.set_ylabel('iteration')
ax2.set_ylabel('iteration')
plt.show()

def R_squared(w,b,x,y):
  m=x.shape[0]
  sqr_error=0
  mean_sqr_error=0
  mu=np.mean(y,axis=0)

  for i in range(m):
    y_pred=np.dot(x[i],w)+b
    temp_sqr_error=y[i]-y_pred**2
    sqr_error+=temp_sqr_error

    temp_mean_sqr_error=(y[i]-mu)**2
    mean_sqr_error+=temp_mean_sqr_error

    R_squared=1-(sqr_error/mean_sqr_error)
    return R_squared

accuracy=R_squared(w_final,b_final,x_train,y_train)
print(accuracy)

y_pred=np.dot(x_test[1],w_final)+b_final
print(y_pred,y_test[1])

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score

lr=LinearRegression()
model=lr.fit(x_train,y_train)
y_pred=model.predict(x_test)

score=model.score(x_test,y_test)
print(score)
mse=mean_squared_error(y_test,y_pred)
r2=r2_score(y_test,y_pred)
print(r2)